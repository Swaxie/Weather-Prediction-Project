{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05033800-6d67-4d78-ac0f-c3b198c1f959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Daily Historical Updates Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3418a11a-67f2-4527-a767-0b9e90f7f667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setting up the glue session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5d275d-0ca0-42bd-a072-a5ffa371469a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openmeteo-requests\n  Obtaining dependency information for openmeteo-requests from https://files.pythonhosted.org/packages/13/31/96209383687cf35055eb628e3a9207a07ac2d5faf6e70076f459435a989e/openmeteo_requests-1.3.0-py3-none-any.whl.metadata\n  Downloading openmeteo_requests-1.3.0-py3-none-any.whl.metadata (9.7 kB)\nCollecting openmeteo-sdk>=1.4.0 (from openmeteo-requests)\n  Obtaining dependency information for openmeteo-sdk>=1.4.0 from https://files.pythonhosted.org/packages/18/9a/f33c4eb783d505d0099c039bbac30da09266027d9e3e0b5de76ef796749d/openmeteo_sdk-1.18.0-py3-none-any.whl.metadata\n  Downloading openmeteo_sdk-1.18.0-py3-none-any.whl.metadata (934 bytes)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from openmeteo-requests) (2.31.0)\nCollecting flatbuffers>=24.0.0 (from openmeteo-sdk>=1.4.0->openmeteo-requests)\n  Obtaining dependency information for flatbuffers>=24.0.0 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->openmeteo-requests) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->openmeteo-requests) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests->openmeteo-requests) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->openmeteo-requests) (2023.7.22)\nDownloading openmeteo_requests-1.3.0-py3-none-any.whl (6.0 kB)\nDownloading openmeteo_sdk-1.18.0-py3-none-any.whl (7.6 kB)\nDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\nInstalling collected packages: flatbuffers, openmeteo-sdk, openmeteo-requests\nSuccessfully installed flatbuffers-24.3.25 openmeteo-requests-1.3.0 openmeteo-sdk-1.18.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting requests-cache\n  Obtaining dependency information for requests-cache from https://files.pythonhosted.org/packages/4e/2e/8f4051119f460cfc786aa91f212165bb6e643283b533db572d7b33952bd2/requests_cache-1.2.1-py3-none-any.whl.metadata\n  Downloading requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\nCollecting retry-requests\n  Obtaining dependency information for retry-requests from https://files.pythonhosted.org/packages/b1/f3/8ce908497bebbc2790ef06240a2c0fb28c096abb59062d88f85090464a5f/retry_requests-2.0.0-py3-none-any.whl.metadata\n  Downloading retry_requests-2.0.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.11/site-packages (1.23.5)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (1.5.3)\nCollecting attrs>=21.2 (from requests-cache)\n  Obtaining dependency information for attrs>=21.2 from https://files.pythonhosted.org/packages/89/aa/ab0f7891a01eeb2d2e338ae8fecbe57fcebea1a24dbb64d45801bfab481d/attrs-24.3.0-py3-none-any.whl.metadata\n  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\nCollecting cattrs>=22.2 (from requests-cache)\n  Obtaining dependency information for cattrs>=22.2 from https://files.pythonhosted.org/packages/c8/d5/867e75361fc45f6de75fe277dd085627a9db5ebb511a87f27dc1396b5351/cattrs-24.1.2-py3-none-any.whl.metadata\n  Downloading cattrs-24.1.2-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: platformdirs>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests-cache) (3.10.0)\nRequirement already satisfied: requests>=2.22 in /databricks/python3/lib/python3.11/site-packages (from requests-cache) (2.31.0)\nCollecting url-normalize>=1.4 (from requests-cache)\n  Obtaining dependency information for url-normalize>=1.4 from https://files.pythonhosted.org/packages/65/1c/6c6f408be78692fc850006a2b6dea37c2b8592892534e09996e401efc74b/url_normalize-1.4.3-py2.py3-none-any.whl.metadata\n  Downloading url_normalize-1.4.3-py2.py3-none-any.whl.metadata (3.1 kB)\nRequirement already satisfied: urllib3>=1.25.5 in /databricks/python3/lib/python3.11/site-packages (from requests-cache) (1.26.16)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2022.7)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.22->requests-cache) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.22->requests-cache) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.22->requests-cache) (2023.7.22)\nDownloading requests_cache-1.2.1-py3-none-any.whl (61 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/61.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.4/61.4 kB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading retry_requests-2.0.0-py3-none-any.whl (15 kB)\nDownloading attrs-24.3.0-py3-none-any.whl (63 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/63.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.4/63.4 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading cattrs-24.1.2-py3-none-any.whl (66 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/66.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.4/66.4 kB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\nInstalling collected packages: url-normalize, attrs, retry-requests, cattrs, requests-cache\nSuccessfully installed attrs-24.3.0 cattrs-24.1.2 requests-cache-1.2.1 retry-requests-2.0.0 url-normalize-1.4.3\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install openmeteo-requests\n",
    "%pip install requests-cache retry-requests numpy pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d99963c-c8b7-4745-914e-6e738767a593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e863ad-7a66-4b70-9aae-c71b8d765c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, max as spark_max\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4bbbbae-0866-4c95-91dd-dedcb8b2e324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setting up parameters and requesting data using the weather API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b5903c-ae77-4a0d-b49d-d7c7c7e050f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion will start from: 2024-12-16\nEnd date is set to: 2024-12-15\n"
     ]
    }
   ],
   "source": [
    "# Define S3 bucket details and file path\n",
    "bucket_name = \"de-upskilling-weather\"\n",
    "folder = \"LandingZone\"\n",
    "file_name = \"daily_historical_updates.parquet\"\n",
    "org_file_name = \"daily_historical.parquet\"\n",
    "s3_path = f\"/mnt/{bucket_name}/{folder}/Updates/{file_name}\"\n",
    "\n",
    "df_update = spark.read.parquet(f\"/mnt/{bucket_name}/Gold/log.parquet\")\n",
    "\n",
    "# Convert 'datetime' column to timestamp type\n",
    "df_update = df_update.withColumn(\"latest_data\", F.to_timestamp(\"latest_data\"))\n",
    "\n",
    "# Specify the name you are filtering for\n",
    "specific_name = \"daily_historical.parquet\"\n",
    "\n",
    "# Filter by name and select the row with the maximum datetime\n",
    "last_updated = df_update.filter(F.col(\"file_name\") == org_file_name).orderBy(F.col(\"latest_data\").desc()).limit(1).select('latest_data').collect()[0][0].date()\n",
    "\n",
    "# Convert last_updated to a datetime object\n",
    "#last_updated = datetime.strptime(last_updated, \"%Y-%m-%d\")\n",
    "# Set start_date to one day after the last date\n",
    "start_date = (last_updated + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "end_date = (datetime.today() - timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "print(\"Ingestion will start from:\", start_date)\n",
    "\n",
    "print(\"End date is set to:\", end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e937abd5-506e-46f8-ba8c-eaf21848cde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "latitudes = [38.855584, 40.724039, 25.695215, 34.005712, 32.724548, 47.594472, 39.676938, 39.952583, 39.791, 35.481918, 36.174465, 41.881832, 51.509865, 52.520008, 48.137154, 48.864716, 47.373878, 51.260197, 52.377956, 53.35014, 40.416775, 48.210033, 46.056946, 50.073658, 55.676098]\n",
    "longitudes = [-77.036975, -73.994982, -80.168933, -118.175596, -96.76923, -122.348286, -104.977053, -75.165222, -86.148003, -97.508469, -86.76796, -87.623177, -0.118092, 13.404954, 11.576124, 2.349014, 8.545094, 4.402771, 4.89707, -6.266155, -3.70379, 16.363449, 14.505751, 14.41854, 12.568337]\n",
    "params = {\n",
    "\t\"latitude\": latitudes,\n",
    "\t\"longitude\": longitudes,\n",
    "\t\"start_date\": start_date,\n",
    "\t\"end_date\": end_date,\n",
    "\t\"daily\": [\"temperature_2m_max\", \"temperature_2m_min\", \"temperature_2m_mean\", \"apparent_temperature_max\", \"apparent_temperature_min\", \"apparent_temperature_mean\", \"precipitation_sum\", \"rain_sum\", \"snowfall_sum\", \"wind_speed_10m_max\", \"wind_gusts_10m_max\", \"wind_direction_10m_dominant\"]\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "223f7da0-0200-4ca1-87a1-ae1256af4f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processing the responce\n",
    "* Iterating through the list received\n",
    "* Extracting information\n",
    "* storing the information in a spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c050fa2-607d-4885-8327-8c4fd37515d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "\n",
    "for i, response in enumerate(responses):\n",
    "\n",
    "    #response = responses[0]\n",
    "    #print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "    #print(f\"Elevation {response.Elevation()} m asl\")\n",
    "    #print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "    #print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "    # Process daily data. The order of variables needs to be the same as requested.\n",
    "    daily = response.Daily()\n",
    "    daily_temperature_2m_max = daily.Variables(0).ValuesAsNumpy()\n",
    "    daily_temperature_2m_min = daily.Variables(1).ValuesAsNumpy()\n",
    "    daily_temperature_2m_mean = daily.Variables(2).ValuesAsNumpy()\n",
    "    daily_apparent_temperature_max = daily.Variables(3).ValuesAsNumpy()\n",
    "    daily_apparent_temperature_min = daily.Variables(4).ValuesAsNumpy()\n",
    "    daily_apparent_temperature_mean = daily.Variables(5).ValuesAsNumpy()\n",
    "    daily_precipitation_sum = daily.Variables(6).ValuesAsNumpy()\n",
    "    daily_rain_sum = daily.Variables(7).ValuesAsNumpy()\n",
    "    daily_snowfall_sum = daily.Variables(8).ValuesAsNumpy()\n",
    "    daily_wind_speed_10m_max = daily.Variables(9).ValuesAsNumpy()\n",
    "    daily_wind_gusts_10m_max = daily.Variables(10).ValuesAsNumpy()\n",
    "    daily_wind_direction_10m_dominant = daily.Variables(11).ValuesAsNumpy()\n",
    "\n",
    "    daily_data = {\"date\": pd.date_range(\n",
    "        start = pd.to_datetime(daily.Time(), unit = \"s\", utc = True),\n",
    "        end = pd.to_datetime(daily.TimeEnd(), unit = \"s\", utc = True),\n",
    "        freq = pd.Timedelta(seconds = daily.Interval()),\n",
    "        inclusive = \"left\"\n",
    "    )}\n",
    "    \n",
    "    daily_data[\"latitude\"] = latitudes[i]\n",
    "    daily_data[\"longitude\"] = longitudes[i]\n",
    "    \n",
    "    daily_data[\"temperature_2m_max\"] = daily_temperature_2m_max\n",
    "    daily_data[\"temperature_2m_min\"] = daily_temperature_2m_min\n",
    "    daily_data[\"temperature_2m_mean\"] = daily_temperature_2m_mean\n",
    "    daily_data[\"apparent_temperature_max\"] = daily_apparent_temperature_max\n",
    "    daily_data[\"apparent_temperature_min\"] = daily_apparent_temperature_min\n",
    "    daily_data[\"apparent_temperature_mean\"] = daily_apparent_temperature_mean\n",
    "    daily_data[\"precipitation_sum\"] = daily_precipitation_sum\n",
    "    daily_data[\"rain_sum\"] = daily_rain_sum\n",
    "    daily_data[\"snowfall_sum\"] = daily_snowfall_sum\n",
    "    daily_data[\"wind_speed_10m_max\"] = daily_wind_speed_10m_max\n",
    "    daily_data[\"wind_gusts_10m_max\"] = daily_wind_gusts_10m_max\n",
    "    daily_data[\"wind_direction_10m_dominant\"] = daily_wind_direction_10m_dominant\n",
    "\n",
    "    daily_dataframe = pd.DataFrame(data = daily_data)\n",
    "    \n",
    "    if i == 0:\n",
    "        spark_df = spark.createDataFrame(daily_dataframe)\n",
    "    else:\n",
    "        # Convert `daily_dataframe` to a Spark DataFrame\n",
    "        daily_spark_df = spark.createDataFrame(daily_dataframe)\n",
    "\n",
    "        # Step 2: Concatenate the two Spark DataFrames\n",
    "        spark_df = spark_df.unionByName(daily_spark_df)\n",
    "    \n",
    "    #print(daily_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab98e37a-87cc-4335-901d-a805a7ee5940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- date: timestamp (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- temperature_2m_max: double (nullable = true)\n |-- temperature_2m_min: double (nullable = true)\n |-- temperature_2m_mean: double (nullable = true)\n |-- apparent_temperature_max: double (nullable = true)\n |-- apparent_temperature_min: double (nullable = true)\n |-- apparent_temperature_mean: double (nullable = true)\n |-- precipitation_sum: double (nullable = true)\n |-- rain_sum: double (nullable = true)\n |-- snowfall_sum: double (nullable = true)\n |-- wind_speed_10m_max: double (nullable = true)\n |-- wind_gusts_10m_max: double (nullable = true)\n |-- wind_direction_10m_dominant: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46030edb-99c2-49ae-b382-8d9792e35af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f69c7015-3476-440a-9366-816ed0493698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Writing the df into the S3 bucket LandingZone Folder and updating the last_updated dim table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff90c0b-6621-47c0-a914-7d61130e7e7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame to an S3 bucket in Parquet format\n",
    "spark_df.write.mode(\"overwrite\").parquet(s3_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e64897f5-620b-436a-8448-3a7e5f31321c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+\n|           file_name|latest_data|       date_updated|\n+--------------------+-----------+-------------------+\n|daily_historical....| 2024-12-09|2024-12-11 15:28:18|\n+--------------------+-----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from datetime import datetime\n",
    "\n",
    "# Get current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format date and time as a string\n",
    "formatted_date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (org_file_name, end_date, formatted_date_time)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"latest_data\", StringType(), True),\n",
    "    StructField(\"date_updated\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_log = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df_log.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62526e49-283b-4e50-a369-bcdb74fe2051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_log.write.mode(\"append\").parquet(f\"/mnt/{bucket_name}/Gold/log.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a3e03e8-9b2e-47df-9da5-c7bfd6221bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "daily_historical-updates.ipynb",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}