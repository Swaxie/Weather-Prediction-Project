{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81bd1d80-1917-4ac2-902d-6d74803f3440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, to_timestamp, avg\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c167cd76-d1e0-4ca2-8316-c9bcba52683f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel, GBTRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc7ab89-bb5b-487a-a3ca-3f136cbdbd0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# assembler and feature columns\n",
    "\n",
    "feature_columns = ['temp_lag_1hr', 'rolling_temp_avg3hr', 'temp_lag_3hr', 'rolling_temp_avg6hr', 'dew_point_2m', 'season', 'shortwave_radiation', 'month', 'day_of_year', 'pressure_msl', 'surface_pressure', 'hour', 'cloud_cover', 'temp_lag_1day', 'rolling_temp_avg1day']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f12d85-5507-429e-8a8f-24738d47d507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cities_dim = spark.read.parquet('/mnt/de-upskilling-weather/Silver/cities_dim.parquet/')\n",
    "cities_list = cities_dim.select('city').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d90fa3d-0bc9-42a1-a2ef-bf33554b7be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "month_to_season = {\n",
    "    12: 0, 1: 0, 2: 0,  # Winter\n",
    "    3: 1, 4: 1, 5: 1,  # Spring\n",
    "    6: 2, 7: 2, 8: 2,  # Summer\n",
    "    9: 3, 10: 3, 11: 3  # Fall\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca2e9e8-a9d6-467b-9799-39da0bdcab5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "api_forecasts_df = spark.read.parquet('/mnt/de-upskilling-weather/Gold/7day_hourly_forecast.parquet/')\n",
    "api_forecasts_df = api_forecasts_df.select('date', 'city', 'dew_point_2m', 'shortwave_radiation', 'surface_pressure', 'pressure_msl', 'cloud_cover').orderBy('city','date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e45cc1-0cfa-4b25-a9f7-0ba7cd8e6ce8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Forecast Generation with Lists"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with Vienna\nfinished with Ljubljana\nfinished with Denver\nfinished with Paris\nfinished with Zurich\nfinished with London\nfinished with Berlin\nfinished with Philadelphia\nfinished with Indianapolis\nfinished with Nashville\nfinished with Chicago\nfinished with New York\nfinished with Miami\nfinished with Washington, DC\nfinished with Oklahoma City\nfinished with Dallas\nfinished with Seattle\nfinished with Los Angeles\nfinished with Prague\nfinished with Copenhagen\nfinished with Amsterdam\nfinished with Dublin\nfinished with Antwerp\nfinished with Munich\nfinished with Madrid\n"
     ]
    }
   ],
   "source": [
    "for city in cities_list:\n",
    "\n",
    "    # removing whitespace and reading in dataframe\n",
    "    city_no_ws = re.sub(r'\\W+', '', city)\n",
    "    df = spark.read.parquet(f'/mnt/de-upskilling-weather/Silver/Forecast_Data/{city_no_ws}_forecast_data.parquet/').withColumn('datetime', to_timestamp('datetime'))\n",
    "    api_forecast = api_forecasts_df.where(col('city') == city)\n",
    "\n",
    "    # generating temperature list and converting to np array\n",
    "    temp_list = df.select('temperature_2m').rdd.flatMap(lambda x: x).collect()\n",
    "    np_temp_list = np.array(temp_list)\n",
    "\n",
    "    # loading model\n",
    "    if city in ['Amsterdam', 'Copenhagen', 'Dublin', 'Antwerp']:\n",
    "        model = GBTRegressionModel.load(f'/mnt/de-upskilling-weather/MachineLearning/{city_no_ws}_forecast_model/')\n",
    "    else:\n",
    "        model = LinearRegressionModel.load(f'/mnt/de-upskilling-weather/MachineLearning/{city_no_ws}_forecast_model/')\n",
    "\n",
    "    # used to create forecast dataframe after\n",
    "    ts_list = []\n",
    "    temp_preds = []\n",
    "\n",
    "    # using latest timestamp to generate last row df and keep track\n",
    "    current_ts = df.tail(1)[0]['datetime'] + timedelta(hours=1)\n",
    "\n",
    "    # API forecast variables\n",
    "    api_forecast = api_forecasts_df.where((col('city') == city) & (col('date') >= current_ts))\n",
    "\n",
    "    dew_point_2m_list = api_forecast.select('dew_point_2m').rdd.flatMap(lambda x: x).collect()\n",
    "    shortwave_radiation_list = api_forecast.select('shortwave_radiation').rdd.flatMap(lambda x: x).collect()\n",
    "    surface_pressure_list = api_forecast.select('surface_pressure').rdd.flatMap(lambda x: x).collect()\n",
    "    pressure_msl_list = api_forecast.select('pressure_msl').rdd.flatMap(lambda x: x).collect()\n",
    "    cloud_cover_list = api_forecast.select('cloud_cover').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for i in range(168):\n",
    "        # generating new row\n",
    "        new_row = Row(month=current_ts.month, \n",
    "                    hour=current_ts.hour,\n",
    "                    datetime=current_ts,\n",
    "                    day_of_year=current_ts.timetuple()[7],\n",
    "                    season=month_to_season[current_ts.month],\n",
    "                    temp_lag_1hr=float(np_temp_list[-1]),\n",
    "                    temp_lag_3hr=float(np_temp_list[-3]),\n",
    "                    temp_lag_1day=float(np_temp_list[-24]),\n",
    "                    rolling_temp_avg3hr=float(np.average(np_temp_list[-3:])),\n",
    "                    rolling_temp_avg6hr=float(np.average(np_temp_list[-6:])),\n",
    "                    rolling_temp_avg1day=float(np.average(np_temp_list[-24:])),\n",
    "                    temperature_2m=float('nan'),\n",
    "                    dew_point_2m=dew_point_2m_list[i],\n",
    "                    shortwave_radiation=shortwave_radiation_list[i],\n",
    "                    pressure_msl=pressure_msl_list[i],\n",
    "                    surface_pressure=surface_pressure_list[i],\n",
    "                    cloud_cover=cloud_cover_list[i]\n",
    "                    )\n",
    "        current_row_df = spark.createDataFrame([new_row])\n",
    "\n",
    "        # applying model and getting prediction\n",
    "        prediction_data = assembler.transform(current_row_df).select('features', 'temperature_2m')\n",
    "\n",
    "        prediction = model.transform(prediction_data)\n",
    "        temp_pred = prediction.tail(1)[0][2]\n",
    "\n",
    "        np_temp_list = np.append(np_temp_list[1:], temp_pred)\n",
    "\n",
    "        # adding an hour to latest_ts\n",
    "        ts_list.append(current_ts)\n",
    "        temp_preds.append(temp_pred)\n",
    "        current_ts = current_ts + timedelta(hours=1)\n",
    "\n",
    "    forecast_df = spark.createDataFrame(list(zip(ts_list, temp_preds)), schema=['timestamp', 'pred_temp'])\n",
    "    forecast_df.write.mode('overwrite').parquet(f'/mnt/de-upskilling-weather/Silver/Forecasts/{city_no_ws}_forecast.parquet/')\n",
    "    print(f'finished with {city}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Temperature Forecasting",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}