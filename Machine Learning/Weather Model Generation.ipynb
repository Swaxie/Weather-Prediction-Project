{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d00de27-c33d-4f90-a37f-61e24efd2c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, floor, when, concat_ws, to_timestamp, lpad, lag, lead, avg, hour, dayofyear\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb7f3ad-3b97-429f-b484-813c5122eeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2742b328-0d7a-483b-9c3c-f476363dc8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cities_list = spark.read.parquet('/mnt/de-upskilling-weather/Silver/cities_dim.parquet/').select(\"city\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "hourly_historical = spark.read.parquet('/mnt/de-upskilling-weather/Gold/hourly_historical.parquet/')\n",
    "hourly_historical = hourly_historical.select('city', 'year', 'month', hour('datetime').alias('hour'), 'datetime', 'temperature_2m', 'dew_point_2m', 'shortwave_radiation', 'surface_pressure', 'pressure_msl', 'cloud_cover')\n",
    "hourly_historical = hourly_historical.withColumn('datetime', to_timestamp('datetime')).withColumn('day_of_year', dayofyear('datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4046ec36-9d53-4d77-96fb-c128134404aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Season Feature\n",
    "# Winter(0) = 12, 1, 2\n",
    "# Spring(1) = 3, 4, 5\n",
    "# Summer(2) = 6, 7, 8\n",
    "# Fall(3) = 9, 10, 11\n",
    "hourly_historical = hourly_historical.withColumn(\"season\", when(col(\"month\").isin([12, 1, 2]), 0)\n",
    "                         .when(col(\"month\").isin([3, 4, 5]), 1)\n",
    "                         .when(col(\"month\").isin([6, 7, 8]), 2)\n",
    "                         .otherwise(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e76c2ac-0fd6-4849-9f66-a542a2f7f144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# vectorizing columns and linear regression preparation\n",
    "\n",
    "feature_columns = ['temp_lag_1hr', 'rolling_temp_avg3hr', 'temp_lag_3hr', 'rolling_temp_avg6hr', 'dew_point_2m', 'season', 'shortwave_radiation', 'month', 'day_of_year', 'pressure_msl', 'surface_pressure', 'hour', 'cloud_cover', 'temp_lag_1day', 'rolling_temp_avg1day']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "lr = LinearRegression(featuresCol='features', labelCol='temperature_2m')\n",
    "gbtr = GBTRegressor(labelCol=\"temperature_2m\", featuresCol=\"features\", maxIter=150, maxDepth=7, stepSize=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c67465-beb8-4933-987d-b3fc1c118470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# windows for time series features\n",
    "lag_window = Window.orderBy('datetime')\n",
    "rolling_window_3hr = Window.orderBy('datetime').rowsBetween(-3, -1)\n",
    "rolling_window_6hr = Window.orderBy('datetime').rowsBetween(-6, -1)\n",
    "rolling_window_12hr = Window.orderBy('datetime').rowsBetween(-12, -1)\n",
    "rolling_window_24hr = Window.orderBy('datetime').rowsBetween(-24, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e2fe5d-35a8-461d-b82d-e52200829242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Vienna\nVienna forecast data written\nVienna forecast model written\nworking on Ljubljana\nLjubljana forecast data written\nLjubljana forecast model written\nworking on Denver\nDenver forecast data written\nDenver forecast model written\nworking on Paris\nParis forecast data written\nParis forecast model written\nworking on Zurich\nZurich forecast data written\nZurich forecast model written\nworking on London\nLondon forecast data written\nLondon forecast model written\nworking on Berlin\nBerlin forecast data written\nBerlin forecast model written\nworking on Philadelphia\nPhiladelphia forecast data written\nPhiladelphia forecast model written\nworking on Indianapolis\nIndianapolis forecast data written\nIndianapolis forecast model written\nworking on Nashville\nNashville forecast data written\nNashville forecast model written\nworking on Chicago\nChicago forecast data written\nChicago forecast model written\nworking on New York\nNew York forecast data written\nNew York forecast model written\nworking on Miami\nMiami forecast data written\nMiami forecast model written\nworking on Washington, DC\nWashington, DC forecast data written\nWashington, DC forecast model written\nworking on Oklahoma City\nOklahoma City forecast data written\nOklahoma City forecast model written\nworking on Dallas\nDallas forecast data written\nDallas forecast model written\nworking on Seattle\nSeattle forecast data written\nSeattle forecast model written\nworking on Los Angeles\nLos Angeles forecast data written\nLos Angeles forecast model written\nworking on Prague\nPrague forecast data written\nPrague forecast model written\nworking on Copenhagen\nCopenhagen forecast data written\nUsing GBT Regressor\nCopenhagen forecast model written\nworking on Amsterdam\nAmsterdam forecast data written\nUsing GBT Regressor\nAmsterdam forecast model written\nworking on Dublin\nDublin forecast data written\nUsing GBT Regressor\nDublin forecast model written\nworking on Antwerp\nAntwerp forecast data written\nUsing GBT Regressor\nAntwerp forecast model written\nworking on Munich\nMunich forecast data written\nMunich forecast model written\nworking on Madrid\nMadrid forecast data written\nMadrid forecast model written\n"
     ]
    }
   ],
   "source": [
    "# generating models for each city\n",
    "for city in cities_list:\n",
    "    print(f'working on {city}') \n",
    "    city_no_whitespace = re.sub(r'\\W+', '', city)\n",
    "    model_path = f'/mnt/de-upskilling-weather/MachineLearning/{city_no_whitespace}_forecast_model'\n",
    "    forecast_data_path = f'/mnt/de-upskilling-weather/Silver/Forecast_Data/{city_no_whitespace}_forecast_data.parquet'\n",
    "\n",
    "    # getting dataset for specific city\n",
    "    df = hourly_historical.where(col('city') == city)\n",
    "    df = df.drop('city').orderBy('datetime')\n",
    "\n",
    "    # temperature lags\n",
    "    df = df.withColumn('temp_lag_1hr', lag('temperature_2m', 1).over(lag_window))\n",
    "    df = df.withColumn('temp_lag_3hr', lag('temperature_2m', 3).over(lag_window))\n",
    "    df = df.withColumn('temp_lag_1day', lag('temperature_2m', 24).over(lag_window))\n",
    "\n",
    "    # temperature rolling averages\n",
    "    df = df.withColumn('rolling_temp_avg3hr', avg('temperature_2m').over(rolling_window_3hr))\n",
    "    df = df.withColumn('rolling_temp_avg6hr', avg('temperature_2m').over(rolling_window_6hr))\n",
    "    df = df.withColumn('rolling_temp_avg1day', avg('temperature_2m').over(rolling_window_24hr))\n",
    "\n",
    "    # getting last 25 rows for forecasting purposes\n",
    "    forecast_data = df.orderBy('datetime', ascending=False).limit(25).orderBy('datetime')\n",
    "    forecast_data.write.mode('overwrite').parquet(forecast_data_path)\n",
    "\n",
    "    print(f'{city} forecast data written')\n",
    "\n",
    "    # dropping all nulls created from lags\n",
    "    df = df.dropna()\n",
    "\n",
    "    # vectorizing features, generating model, and saving model\n",
    "    train_data = assembler.transform(df).select('features', 'temperature_2m')\n",
    "\n",
    "    # check for cities that need to be trained with a GBT Regressor\n",
    "    if city in ['Amsterdam', 'Copenhagen', 'Dublin', 'Antwerp']:\n",
    "        print('Using GBT Regressor')\n",
    "        forecast_model = gbtr.fit(train_data)\n",
    "    else:\n",
    "        forecast_model = lr.fit(train_data)\n",
    "\n",
    "    forecast_model.write().overwrite().save(model_path)\n",
    "    print(f'{city} forecast model written')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Weather Model Generation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}