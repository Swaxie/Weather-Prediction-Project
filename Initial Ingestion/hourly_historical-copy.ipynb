{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c930d034-9ef7-4f60-84d1-ddf9bb668007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "editable": true,
    "trusted": true
   },
   "source": [
    "# AWS Glue Studio Notebook\n",
    "##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c283527-f5ff-4653-8993-65d7a83fb88e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "editable": true,
    "trusted": true
   },
   "source": [
    "#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ea880db-17ea-4c83-8b1f-c0536e2a0dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nCurrent timeout is None minutes.\ntimeout has been set to 120 minutes.\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 15 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nAdditional python modules to be included:\nopenmeteo_requests\nrequests_cache\nretry_requests\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 15\nTimeout: 120\nSession ID: 4345512e-643d-40bf-ab61-f6fae508d459\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\n--additional-python-modules openmeteo_requests,requests_cache,retry_requests\nWaiting for session 4345512e-643d-40bf-ab61-f6fae508d459 to get into ready status...\nSession 4345512e-643d-40bf-ab61-f6fae508d459 has been created.\n\n"
     ]
    }
   ],
   "source": [
    "#%pip install openmeteo-requests\n",
    "#%pip install requests-cache retry-requests numpy pandas\n",
    "\n",
    "\n",
    "\n",
    "%timeout 120\n",
    "%idle_timeout 15\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2\n",
    "\n",
    "%additional_python_modules openmeteo_requests, requests_cache, retry_requests\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "#%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d21f8bc7-a086-48e4-a6b8-0bb0306f63f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, max as spark_max\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb5c42f-ab3e-41e8-9698-0218e91775b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion will start from: 2010-01-01\nEnd date is set to: 2024-11-10\n"
     ]
    }
   ],
   "source": [
    "# Define S3 bucket details and file path\n",
    "bucket_name = \"de-upskill-weatherforecasting\"\n",
    "folder = \"LandingZone\"\n",
    "file_name = \"hourly_historical.parquet\"\n",
    "s3_path = f\"s3://{bucket_name}/{folder}/{file_name}\"\n",
    "\n",
    "# Default start date\n",
    "default_start_date = \"2010-01-01\"\n",
    "\n",
    "\n",
    "# Try to load the existing data from S3\n",
    "df_update = spark.read.parquet(f\"s3://{bucket_name}/Gold/last_updated.parquet\")\n",
    "df_update.cache()\n",
    "\n",
    "# Get the day data was lasr received\n",
    "last_updated = df_update.filter(df_update.file_name == file_name).select('last_updated').collect()[0][0]\n",
    "\n",
    "\n",
    "# Convert last_updated to a datetime object\n",
    "last_updated = datetime.strptime(last_updated, \"%Y-%m-%d\")\n",
    "# Set start_date to one day after the last date\n",
    "start_date = (last_updated + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "print(\"Ingestion will start from:\", start_date)\n",
    "\n",
    "print(\"End date is set to:\", end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aff4097-2b17-47e8-8ab5-4af9658c8016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "latitudes = [38.855584, 40.724039, 25.695215, 34.005712, 32.724548, 47.594472, 39.676938, 39.952583, 39.791, 35.481918, 36.174465, 41.881832, 51.509865, 52.520008, 48.137154, 48.864716, 47.373878, 51.260197, 52.377956, 53.35014, 40.416775, 48.210033, 46.056946, 50.073658, 55.676098]\n",
    "longitudes = [-77.036975, -73.994982, -80.168933, -118.175596, -96.76923, -122.348286, -104.977053, -75.165222, -86.148003, -97.508469, -86.76796, -87.623177, -0.118092, 13.404954, 11.576124, 2.349014, 8.545094, 4.402771, 4.89707, -6.266155, -3.70379, 16.363449, 14.505751, 14.41854, 12.568337]\n",
    "timezones = [\"America/New_York\", \"America/New_York\", \"America/New_York\", \"America/Los_Angeles\", \"America/Chicago\", \"America/Los_Angeles\", \"America/Denver\", \"America/New_York\", \"America/Indiana/Indianapolis\", \"America/Chicago\", \"America/Chicago\", \"America/Chicago\", \"Europe/London\", \"Europe/Berlin\", \"Europe/Berlin\", \"Europe/Paris\", \"Europe/Zurich\", \"Europe/Brussels\", \"Europe/Amsterdam\", \"Europe/Dublin\", \"Europe/Madrid\", \"Europe/Vienna\", \"Europe/Ljubljana\", \"Europe/Prague\", \"Europe/Copenhagen\"]\n",
    "\n",
    "params = {\n",
    "\t\"latitude\": latitudes,\n",
    "\t\"longitude\": longitudes,\n",
    "\t\"start_date\": start_date,\n",
    "\t\"end_date\": end_date,\n",
    "\t\"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\", \"snowfall\", \"snow_depth\", \"weather_code\", \"wind_speed_10m\", \"wind_speed_100m\", \"wind_direction_10m\", \"wind_direction_100m\", \"wind_gusts_10m\"],\n",
    "\t\"timezone\": timezones\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)# Process first location. Add a for-loop for multiple locations or weather models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7810964c-5acb-4bad-a894-f42d9519e10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 38.84006881713867°N -77.09014892578125°E\nElevation 3.0 m asl\nTimezone b'America/New_York' b'EST'\nTimezone difference to GMT+0 -18000 s\nCoordinates 40.738136291503906°N -74.04254150390625°E\nElevation 28.0 m asl\nTimezone b'America/New_York' b'EST'\nTimezone difference to GMT+0 -18000 s\nCoordinates 25.694198608398438°N -80.3046875°E\nElevation 4.0 m asl\nTimezone b'America/New_York' b'EST'\nTimezone difference to GMT+0 -18000 s\nCoordinates 33.98945236206055°N -118.20223999023438°E\nElevation 50.0 m asl\nTimezone b'America/Los_Angeles' b'PST'\nTimezone difference to GMT+0 -28800 s\nCoordinates 32.72407531738281°N -96.81317138671875°E\nElevation 125.0 m asl\nTimezone b'America/Chicago' b'CST'\nTimezone difference to GMT+0 -21600 s\nCoordinates 47.62741470336914°N -122.32290649414062°E\nElevation 0.0 m asl\nTimezone b'America/Los_Angeles' b'PST'\nTimezone difference to GMT+0 -28800 s\nCoordinates 39.68365478515625°N -105.0°E\nElevation 1628.0 m asl\nTimezone b'America/Denver' b'MST'\nTimezone difference to GMT+0 -25200 s\nCoordinates 39.964847564697266°N -75.1676025390625°E\nElevation 32.0 m asl\nTimezone b'America/New_York' b'EST'\nTimezone difference to GMT+0 -18000 s\nCoordinates 39.82425308227539°N -86.11419677734375°E\nElevation 220.0 m asl\nTimezone b'America/Indiana/Indianapolis' b'EST'\nTimezone difference to GMT+0 -18000 s\nCoordinates 35.465728759765625°N -97.5°E\nElevation 378.0 m asl\nTimezone b'America/Chicago' b'CST'\nTimezone difference to GMT+0 -21600 s\nCoordinates 36.16871643066406°N -86.72726440429688°E\nElevation 131.0 m asl\nTimezone b'America/Chicago' b'CST'\nTimezone difference to GMT+0 -21600 s\nCoordinates 41.8629150390625°N -87.64877319335938°E\nElevation 185.0 m asl\nTimezone b'America/Chicago' b'CST'\nTimezone difference to GMT+0 -21600 s\nCoordinates 51.49384689331055°N -0.16302490234375°E\nElevation 14.0 m asl\nTimezone b'Europe/London' None\nTimezone difference to GMT+0 0 s\nCoordinates 52.5483283996582°N 13.407821655273438°E\nElevation 38.0 m asl\nTimezone b'Europe/Berlin' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 48.1195068359375°N 11.550000190734863°E\nElevation 526.0 m asl\nTimezone b'Europe/Berlin' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 48.892791748046875°N 2.292020559310913°E\nElevation 47.0 m asl\nTimezone b'Europe/Paris' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 47.34621810913086°N 8.54337215423584°E\nElevation 422.0 m asl\nTimezone b'Europe/Zurich' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 51.28295135498047°N 4.378378391265869°E\nElevation 4.0 m asl\nTimezone b'Europe/Brussels' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 52.40773010253906°N 4.842300891876221°E\nElevation 12.0 m asl\nTimezone b'Europe/Amsterdam' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 53.32161331176758°N -6.330810546875°E\nElevation 17.0 m asl\nTimezone b'Europe/Dublin' None\nTimezone difference to GMT+0 0 s\nCoordinates 40.38664245605469°N -3.67608642578125°E\nElevation 666.0 m asl\nTimezone b'Europe/Madrid' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 48.18980407714844°N 16.377296447753906°E\nElevation 193.0 m asl\nTimezone b'Europe/Vienna' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 46.080841064453125°N 14.451510429382324°E\nElevation 299.0 m asl\nTimezone b'Europe/Ljubljana' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 50.08787155151367°N 14.47552490234375°E\nElevation 221.0 m asl\nTimezone b'Europe/Prague' b'CET'\nTimezone difference to GMT+0 3600 s\nCoordinates 55.641475677490234°N 12.596348762512207°E\nElevation 10.0 m asl\nTimezone b'Europe/Copenhagen' b'CET'\nTimezone difference to GMT+0 3600 s\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"
     ]
    }
   ],
   "source": [
    "for i, response in enumerate(responses):\n",
    "\n",
    "    #response = responses[0]\n",
    "    #print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "    #print(f\"Elevation {response.Elevation()} m asl\")\n",
    "    #print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "    #print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "    # Process hourly data. The order of variables needs to be the same as requested.\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "    hourly_precipitation = hourly.Variables(2).ValuesAsNumpy()\n",
    "    hourly_rain = hourly.Variables(3).ValuesAsNumpy()\n",
    "    hourly_snowfall = hourly.Variables(4).ValuesAsNumpy()\n",
    "    hourly_snow_depth = hourly.Variables(5).ValuesAsNumpy()\n",
    "    hourly_weather_code = hourly.Variables(6).ValuesAsNumpy()\n",
    "    hourly_wind_speed_10m = hourly.Variables(7).ValuesAsNumpy()\n",
    "    hourly_wind_speed_100m = hourly.Variables(8).ValuesAsNumpy()\n",
    "    hourly_wind_direction_10m = hourly.Variables(9).ValuesAsNumpy()\n",
    "    hourly_wind_direction_100m = hourly.Variables(10).ValuesAsNumpy()\n",
    "    hourly_wind_gusts_10m = hourly.Variables(11).ValuesAsNumpy()\n",
    "\n",
    "    hourly_data = {\"date\": pd.date_range(\n",
    "        start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "        end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "        freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "        inclusive = \"left\"\n",
    "    )}\n",
    "\n",
    "    hourly_data[\"latitude\"] = latitudes[i]\n",
    "    hourly_data[\"longitude\"] = longitudes[i]\n",
    "    hourly_data[\"timezone\"] = timezones[i]\n",
    "\n",
    "    hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "    hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
    "    hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "    hourly_data[\"rain\"] = hourly_rain\n",
    "    hourly_data[\"snowfall\"] = hourly_snowfall\n",
    "    hourly_data[\"snow_depth\"] = hourly_snow_depth\n",
    "    hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "    hourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
    "    hourly_data[\"wind_speed_100m\"] = hourly_wind_speed_100m\n",
    "    hourly_data[\"wind_direction_10m\"] = hourly_wind_direction_10m\n",
    "    hourly_data[\"wind_direction_100m\"] = hourly_wind_direction_100m\n",
    "    hourly_data[\"wind_gusts_10m\"] = hourly_wind_gusts_10m\n",
    "\n",
    "    hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "    \n",
    "    if i == 0:\n",
    "        # Convert `daily_dataframe` to a Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(hourly_dataframe)\n",
    "    else:\n",
    "        # Convert `daily_dataframe` to a Spark DataFrame\n",
    "        hourly_spark_df = spark.createDataFrame(hourly_dataframe)\n",
    "\n",
    "        # Concatenate the two Spark DataFrames\n",
    "        spark_df = spark_df.unionByName(hourly_spark_df)\n",
    "    \n",
    "    \n",
    "    #print(hourly_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c448805-597f-4a85-9f3e-0bd4e3249c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#129912 per location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40701a16-9cef-4844-8ef3-0c3e2467c284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spark_df.show()  # Displays the Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993b9d2b-5725-4bf2-9ffd-97416f64d5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e38d2eed-916e-490f-98d8-2995f49e7c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3256800\n"
     ]
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc5a8d7-b5f8-4820-af95-eb7e168a4660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.write.mode(\"append\").parquet(f\"s3://{bucket_name}/LandingZone/hourly_historical.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fb53926-576e-4836-9e1e-bb7259e5af63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n|           file_name|last_updated|\n+--------------------+------------+\n|hourly_historical...|            |\n|7day_daily_foreca...|            |\n|daily_historical_...|            |\n|7day_hourly_forec...|            |\n|daily_historical....|            |\n|hourly_historical...|  2024-11-10|\n+--------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# update the last_updated df\n",
    "# Set 'last_updated' to null where file_name is 'file1.parquet'\n",
    "df_update = df_update.withColumn(\"last_updated\",when(col(\"file_name\") == file_name, F.lit(end_date)).otherwise(col(\"last_updated\")))\n",
    "df_update.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "528f6a13-7fa6-41a7-a8fd-611106139970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_update.write.mode(\"overwrite\").parquet(f\"s3://{bucket_name}/Gold/last_updated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abebca99-862e-4d64-bbe9-e1a4125c2401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hourly_historical-copy.ipynb",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}